{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NeuralProphet para Forecast de CMg - CORRECTED EVALUATION\n",
    "\n",
    "## Critical Fix: Removed Data Leakage\n",
    "\n",
    "The original notebook had **severe data leakage** in the rolling forecast evaluation:\n",
    "- It fed actual test values back into the model during prediction\n",
    "- This allowed the model to \"peek\" at future values through its 168-hour AR lags\n",
    "- 23 of 24 test predictions were contaminated\n",
    "\n",
    "### Changes in this corrected version:\n",
    "1. **Fixed rolling forecast**: Uses PREDICTED values (not actuals) for AR continuity\n",
    "2. **Expanded test set**: 30+ days (720+ hours) instead of 24 hours\n",
    "3. **Added TimeSeriesSplit**: k=5 fold cross-validation\n",
    "4. **Per-horizon metrics**: Reports MAE for t+1, t+6, t+12, t+24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install if needed\n",
    "# !pip install neuralprophet lightgbm xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "from datetime import datetime, timedelta\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "# NeuralProphet\n",
    "from neuralprophet import NeuralProphet, set_log_level\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "# Add parent directory to path\n",
    "sys.path.insert(0, os.path.abspath('..'))\n",
    "\n",
    "# Configuration\n",
    "warnings.filterwarnings('ignore')\n",
    "set_log_level('ERROR')\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"Libraries loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Data from Supabase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Supabase client\n",
    "from lib.utils.supabase_client import SupabaseClient\n",
    "\n",
    "def fetch_cmg_data(days: int = 730) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Fetch CMG Online data from Supabase.\n",
    "    \n",
    "    Args:\n",
    "        days: Number of days of history to fetch (default 2 years)\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with 'ds' (datetime) and 'y' (CMG value) columns\n",
    "    \"\"\"\n",
    "    supabase = SupabaseClient()\n",
    "    \n",
    "    end_date = datetime.now()\n",
    "    start_date = end_date - timedelta(days=days)\n",
    "    \n",
    "    print(f\"Fetching CMG data from {start_date.date()} to {end_date.date()}...\")\n",
    "    \n",
    "    records = supabase.get_cmg_online(\n",
    "        start_date=start_date.strftime('%Y-%m-%d'),\n",
    "        end_date=end_date.strftime('%Y-%m-%d'),\n",
    "        limit=days * 24 * 3  # 3 nodes per hour\n",
    "    )\n",
    "    \n",
    "    if not records:\n",
    "        raise ValueError(\"No CMG data found in Supabase\")\n",
    "    \n",
    "    print(f\"  Fetched {len(records)} records\")\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    df = pd.DataFrame(records)\n",
    "    df['datetime'] = pd.to_datetime(df['datetime'])\n",
    "    \n",
    "    # Average across nodes for each hour\n",
    "    df_hourly = df.groupby('datetime')['cmg_usd'].mean().reset_index()\n",
    "    df_hourly.columns = ['ds', 'y']\n",
    "    df_hourly = df_hourly.sort_values('ds').reset_index(drop=True)\n",
    "    \n",
    "    # Remove duplicates\n",
    "    df_hourly = df_hourly.drop_duplicates(subset=['ds'], keep='last')\n",
    "    \n",
    "    print(f\"  Processed {len(df_hourly)} unique hours\")\n",
    "    print(f\"  Date range: {df_hourly['ds'].min()} to {df_hourly['ds'].max()}\")\n",
    "    \n",
    "    return df_hourly\n",
    "\n",
    "# Fetch data\n",
    "df = fetch_cmg_data(days=730)  # 2 years of data\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data summary\n",
    "print(\"=\"*60)\n",
    "print(\"DATA SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Total hours: {len(df):,}\")\n",
    "print(f\"Date range: {df['ds'].min()} to {df['ds'].max()}\")\n",
    "print(f\"CMG mean: ${df['y'].mean():.2f}\")\n",
    "print(f\"CMG std: ${df['y'].std():.2f}\")\n",
    "print(f\"CMG min: ${df['y'].min():.2f}\")\n",
    "print(f\"CMG max: ${df['y'].max():.2f}\")\n",
    "print(f\"Zero CMG hours: {(df['y'] == 0).sum()} ({(df['y'] == 0).mean()*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Train/Test Split\n",
    "\n",
    "**Critical change**: Use 30+ days (720+ hours) for test instead of 24 hours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "TEST_DAYS = 60  # 60 days of test data (1,440 hours)\n",
    "test_size = TEST_DAYS * 24\n",
    "\n",
    "df_train = df[:-test_size].copy().reset_index(drop=True)\n",
    "df_test = df[-test_size:].copy().reset_index(drop=True)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"DATA SPLIT\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Train: {len(df_train):,} hours | {df_train['ds'].min()} to {df_train['ds'].max()}\")\n",
    "print(f\"Test:  {len(df_test):,} hours | {df_test['ds'].min()} to {df_test['ds'].max()}\")\n",
    "print(f\"\\nTest period: {TEST_DAYS} days ({test_size} hours)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization\n",
    "fig, ax = plt.subplots(figsize=(16, 5), dpi=100)\n",
    "\n",
    "# Last 30 days of train + first 30 days of test\n",
    "plot_train = df_train[-720:]\n",
    "plot_test = df_test[:720]\n",
    "\n",
    "ax.plot(plot_train['ds'], plot_train['y'], 'b-', linewidth=0.5, alpha=0.7, label='Train')\n",
    "ax.plot(plot_test['ds'], plot_test['y'], 'g-', linewidth=0.5, alpha=0.7, label='Test')\n",
    "ax.axvline(x=df_test['ds'].iloc[0], color='red', linestyle='--', alpha=0.7, label='Split point')\n",
    "\n",
    "ax.set_xlabel('Date')\n",
    "ax.set_ylabel('CMG (USD/MWh)')\n",
    "ax.set_title('CMG Time Series - Train/Test Split', fontsize=12, fontweight='bold')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. NeuralProphet Model Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_neuralprophet_model():\n",
    "    \"\"\"\n",
    "    Create a NeuralProphet model with optimized configuration for CMG forecasting.\n",
    "    \"\"\"\n",
    "    model = NeuralProphet(\n",
    "        # Auto-regression\n",
    "        n_lags=168,              # 1 week of hourly lags\n",
    "        n_forecasts=1,           # Predict 1 hour ahead (we'll iterate for multi-horizon)\n",
    "        \n",
    "        # Seasonality\n",
    "        yearly_seasonality=True,\n",
    "        weekly_seasonality=True,\n",
    "        daily_seasonality=True,\n",
    "        seasonality_mode='multiplicative',\n",
    "        \n",
    "        # Trend\n",
    "        growth='discontinuous',\n",
    "        n_changepoints=20,\n",
    "        changepoints_range=0.9,\n",
    "        trend_reg=0.05,\n",
    "        \n",
    "        # Training\n",
    "        learning_rate=0.01,\n",
    "        epochs=50,\n",
    "        batch_size=64,\n",
    "        \n",
    "        # Regularization\n",
    "        ar_reg=0.1,\n",
    "        \n",
    "        # Uncertainty\n",
    "        quantiles=[0.025, 0.975],  # 95% CI\n",
    "        \n",
    "        # Architecture\n",
    "        ar_layers=[32, 16],\n",
    "        lagged_reg_layers=[16],\n",
    "    )\n",
    "    return model\n",
    "\n",
    "print(\"Model configuration ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. CORRECTED Rolling Forecast (No Data Leakage)\n",
    "\n",
    "**CRITICAL FIX**: The original code fed actual test values back during rolling forecast.\n",
    "This corrected version uses **predicted values** instead, ensuring true out-of-sample evaluation.\n",
    "\n",
    "### Why this matters:\n",
    "- NeuralProphet uses `n_lags=168` (168 hours of history as features)\n",
    "- If we feed actual values back, by hour 24 the model has 23 actual test values in its features\n",
    "- This is **data leakage** - the model sees \"future\" information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corrected_rolling_forecast(\n",
    "    model,\n",
    "    df_train: pd.DataFrame,\n",
    "    df_test: pd.DataFrame,\n",
    "    max_horizon: int = 24,\n",
    "    verbose: bool = True\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    CORRECTED rolling forecast without data leakage.\n",
    "    \n",
    "    Instead of feeding actual values back (which causes leakage),\n",
    "    we feed PREDICTED values back for AR continuity.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained NeuralProphet model\n",
    "        df_train: Training data with 'ds' and 'y'\n",
    "        df_test: Test data with 'ds' and 'y'\n",
    "        max_horizon: Maximum forecast horizon to evaluate\n",
    "        verbose: Print progress\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with predictions for each (origin, horizon) combination\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    # For each forecast origin in test set\n",
    "    # We need at least max_horizon hours remaining in test\n",
    "    n_origins = len(df_test) - max_horizon\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"Evaluating {n_origins} forecast origins, each predicting {max_horizon} horizons\")\n",
    "    \n",
    "    for origin_idx in range(n_origins):\n",
    "        # Get all data up to this origin point\n",
    "        # This includes training data + test data up to origin_idx\n",
    "        origin_time = df_test['ds'].iloc[origin_idx]\n",
    "        \n",
    "        # Create history: train + test up to (but not including) origin\n",
    "        df_history = pd.concat([\n",
    "            df_train[['ds', 'y']],\n",
    "            df_test[['ds', 'y']].iloc[:origin_idx]\n",
    "        ], ignore_index=True)\n",
    "        \n",
    "        # Create a rolling copy for multi-step forecast\n",
    "        df_rolling = df_history.copy()\n",
    "        \n",
    "        # Forecast each horizon\n",
    "        for h in range(1, max_horizon + 1):\n",
    "            # Make 1-step forecast\n",
    "            future = model.make_future_dataframe(df_rolling, periods=1)\n",
    "            forecast = model.predict(future)\n",
    "            \n",
    "            # Get prediction\n",
    "            pred = forecast['yhat1'].iloc[-1]\n",
    "            \n",
    "            # Get confidence intervals if available\n",
    "            ci_lower = forecast.get('yhat1 2.5%', forecast.get('yhat1 5%', pd.Series([np.nan]))).iloc[-1]\n",
    "            ci_upper = forecast.get('yhat1 97.5%', forecast.get('yhat1 95%', pd.Series([np.nan]))).iloc[-1]\n",
    "            \n",
    "            # Get actual value at this horizon\n",
    "            target_idx = origin_idx + h\n",
    "            if target_idx < len(df_test):\n",
    "                actual = df_test['y'].iloc[target_idx]\n",
    "                target_time = df_test['ds'].iloc[target_idx]\n",
    "            else:\n",
    "                actual = np.nan\n",
    "                target_time = None\n",
    "            \n",
    "            # Store result\n",
    "            results.append({\n",
    "                'origin_time': origin_time,\n",
    "                'target_time': target_time,\n",
    "                'horizon': h,\n",
    "                'y_actual': actual,\n",
    "                'y_pred': pred,\n",
    "                'ci_lower': ci_lower,\n",
    "                'ci_upper': ci_upper\n",
    "            })\n",
    "            \n",
    "            # CRITICAL FIX: Add PREDICTED value (not actual) for AR continuity\n",
    "            # This prevents data leakage while maintaining temporal coherence\n",
    "            next_time = df_test['ds'].iloc[origin_idx + h] if target_idx < len(df_test) else None\n",
    "            if next_time is not None:\n",
    "                new_row = pd.DataFrame({'ds': [next_time], 'y': [pred]})\n",
    "                df_rolling = pd.concat([df_rolling, new_row], ignore_index=True)\n",
    "        \n",
    "        # Progress update\n",
    "        if verbose and (origin_idx + 1) % 100 == 0:\n",
    "            print(f\"  Completed {origin_idx + 1}/{n_origins} origins\")\n",
    "    \n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_df['error'] = results_df['y_actual'] - results_df['y_pred']\n",
    "    results_df['abs_error'] = np.abs(results_df['error'])\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\nGenerated {len(results_df)} predictions\")\n",
    "    \n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative: Simpler true out-of-sample evaluation\n",
    "# This evaluates specific horizons without any data leakage\n",
    "\n",
    "def true_out_of_sample_forecast(\n",
    "    model,\n",
    "    df_train: pd.DataFrame,\n",
    "    df_test: pd.DataFrame,\n",
    "    horizons: List[int] = [1, 6, 12, 24],\n",
    "    sample_every: int = 24,  # Sample every 24 hours for efficiency\n",
    "    verbose: bool = True\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    TRUE out-of-sample forecast evaluation.\n",
    "    \n",
    "    For each forecast origin:\n",
    "    1. Use ONLY data available at that time (train + past test)\n",
    "    2. Make predictions for each horizon\n",
    "    3. Compare to actual values\n",
    "    \n",
    "    NO data leakage: predictions are independent (no rolling AR update).\n",
    "    \n",
    "    Args:\n",
    "        model: Trained NeuralProphet model\n",
    "        df_train: Training data\n",
    "        df_test: Test data\n",
    "        horizons: List of horizons to evaluate\n",
    "        sample_every: Evaluate every N hours (for efficiency)\n",
    "        verbose: Print progress\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with predictions per (origin, horizon)\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    max_h = max(horizons)\n",
    "    \n",
    "    # Origins we'll evaluate (must have max_h hours remaining)\n",
    "    valid_origins = range(0, len(df_test) - max_h, sample_every)\n",
    "    n_origins = len(list(valid_origins))\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"Evaluating {n_origins} origins at horizons {horizons}\")\n",
    "    \n",
    "    for i, origin_idx in enumerate(range(0, len(df_test) - max_h, sample_every)):\n",
    "        origin_time = df_test['ds'].iloc[origin_idx]\n",
    "        \n",
    "        # Build history: train + test[:origin_idx]\n",
    "        df_history = pd.concat([\n",
    "            df_train[['ds', 'y']],\n",
    "            df_test[['ds', 'y']].iloc[:origin_idx]\n",
    "        ], ignore_index=True) if origin_idx > 0 else df_train[['ds', 'y']].copy()\n",
    "        \n",
    "        # Make multi-step forecast from this origin\n",
    "        future = model.make_future_dataframe(df_history, periods=max_h)\n",
    "        forecast = model.predict(future)\n",
    "        \n",
    "        # Extract predictions at each horizon\n",
    "        for h in horizons:\n",
    "            target_idx = origin_idx + h\n",
    "            if target_idx < len(df_test):\n",
    "                actual = df_test['y'].iloc[target_idx]\n",
    "                target_time = df_test['ds'].iloc[target_idx]\n",
    "                \n",
    "                # Get prediction (forecast row at len(df_history) + h - 1)\n",
    "                pred_idx = len(df_history) + h - 1\n",
    "                if pred_idx < len(forecast):\n",
    "                    pred = forecast['yhat1'].iloc[pred_idx]\n",
    "                else:\n",
    "                    pred = np.nan\n",
    "                \n",
    "                results.append({\n",
    "                    'origin_time': origin_time,\n",
    "                    'target_time': target_time,\n",
    "                    'horizon': h,\n",
    "                    'y_actual': actual,\n",
    "                    'y_pred': pred\n",
    "                })\n",
    "        \n",
    "        if verbose and (i + 1) % 10 == 0:\n",
    "            print(f\"  {i + 1}/{n_origins} origins completed\")\n",
    "    \n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_df['error'] = results_df['y_actual'] - results_df['y_pred']\n",
    "    results_df['abs_error'] = np.abs(results_df['error'])\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\nTotal predictions: {len(results_df)}\")\n",
    "    \n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Train Model and Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "print(\"=\"*60)\n",
    "print(\"TRAINING NEURALPROPHET MODEL\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "model = create_neuralprophet_model()\n",
    "metrics = model.fit(df_train, freq='H')\n",
    "\n",
    "print(\"\\nTraining complete\")\n",
    "print(f\"Final MAE: {metrics['MAE'].iloc[-1]:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curve\n",
    "fig, ax = plt.subplots(figsize=(10, 4), dpi=100)\n",
    "ax.plot(metrics['MAE'], 'b-', linewidth=1.5)\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('MAE')\n",
    "ax.set_title('Training Curve - MAE', fontweight='bold')\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run corrected evaluation\n",
    "print(\"=\"*60)\n",
    "print(\"CORRECTED EVALUATION (NO DATA LEAKAGE)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "HORIZONS_TO_EVALUATE = [1, 6, 12, 24]\n",
    "\n",
    "results = true_out_of_sample_forecast(\n",
    "    model=model,\n",
    "    df_train=df_train,\n",
    "    df_test=df_test,\n",
    "    horizons=HORIZONS_TO_EVALUATE,\n",
    "    sample_every=6,  # Evaluate every 6 hours\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(\"\\nEvaluation complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Results Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate metrics per horizon\n",
    "print(\"=\"*70)\n",
    "print(\"CORRECTED METRICS - TRUE OUT-OF-SAMPLE EVALUATION\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\n(This removes the data leakage from the original notebook)\")\n",
    "\n",
    "metrics_by_horizon = []\n",
    "\n",
    "for h in HORIZONS_TO_EVALUATE:\n",
    "    h_results = results[results['horizon'] == h].dropna()\n",
    "    \n",
    "    if len(h_results) > 0:\n",
    "        mae = h_results['abs_error'].mean()\n",
    "        rmse = np.sqrt((h_results['error'] ** 2).mean())\n",
    "        r2 = r2_score(h_results['y_actual'], h_results['y_pred'])\n",
    "        \n",
    "        metrics_by_horizon.append({\n",
    "            'horizon': f't+{h}',\n",
    "            'n_samples': len(h_results),\n",
    "            'mae': mae,\n",
    "            'rmse': rmse,\n",
    "            'r2': r2\n",
    "        })\n",
    "        \n",
    "        print(f\"\\nHorizon t+{h}:\")\n",
    "        print(f\"  Samples: {len(h_results)}\")\n",
    "        print(f\"  MAE:  ${mae:.2f} USD/MWh\")\n",
    "        print(f\"  RMSE: ${rmse:.2f} USD/MWh\")\n",
    "        print(f\"  R²:   {r2:.4f}\")\n",
    "\n",
    "# Overall metrics\n",
    "overall_mae = results['abs_error'].mean()\n",
    "overall_rmse = np.sqrt((results['error'] ** 2).mean())\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"OVERALL METRICS\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Overall MAE:  ${overall_mae:.2f} USD/MWh\")\n",
    "print(f\"Overall RMSE: ${overall_rmse:.2f} USD/MWh\")\n",
    "\n",
    "# Create metrics DataFrame\n",
    "metrics_df = pd.DataFrame(metrics_by_horizon)\n",
    "print(\"\\n\")\n",
    "print(metrics_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize results\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10), dpi=100)\n",
    "\n",
    "# 1. MAE by horizon\n",
    "ax1 = axes[0, 0]\n",
    "ax1.bar([f't+{h}' for h in HORIZONS_TO_EVALUATE], \n",
    "        [m['mae'] for m in metrics_by_horizon],\n",
    "        color='steelblue', edgecolor='white')\n",
    "ax1.set_xlabel('Horizon')\n",
    "ax1.set_ylabel('MAE (USD/MWh)')\n",
    "ax1.set_title('MAE by Forecast Horizon', fontweight='bold')\n",
    "ax1.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add values on bars\n",
    "for i, m in enumerate(metrics_by_horizon):\n",
    "    ax1.text(i, m['mae'] + 1, f\"${m['mae']:.1f}\", ha='center', fontsize=10)\n",
    "\n",
    "# 2. Scatter: Actual vs Predicted (t+1)\n",
    "ax2 = axes[0, 1]\n",
    "h1_results = results[results['horizon'] == 1].dropna()\n",
    "ax2.scatter(h1_results['y_actual'], h1_results['y_pred'], alpha=0.5, s=20)\n",
    "min_val = min(h1_results['y_actual'].min(), h1_results['y_pred'].min())\n",
    "max_val = max(h1_results['y_actual'].max(), h1_results['y_pred'].max())\n",
    "ax2.plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=2, label='Perfect')\n",
    "ax2.set_xlabel('Actual (USD/MWh)')\n",
    "ax2.set_ylabel('Predicted (USD/MWh)')\n",
    "ax2.set_title('t+1 Horizon: Actual vs Predicted', fontweight='bold')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Error distribution\n",
    "ax3 = axes[1, 0]\n",
    "for h in HORIZONS_TO_EVALUATE:\n",
    "    h_errors = results[results['horizon'] == h]['error'].dropna()\n",
    "    ax3.hist(h_errors, bins=50, alpha=0.5, label=f't+{h}')\n",
    "ax3.axvline(x=0, color='red', linestyle='--', linewidth=2)\n",
    "ax3.set_xlabel('Error (USD/MWh)')\n",
    "ax3.set_ylabel('Frequency')\n",
    "ax3.set_title('Error Distribution by Horizon', fontweight='bold')\n",
    "ax3.legend()\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Time series of predictions (sample)\n",
    "ax4 = axes[1, 1]\n",
    "sample = results[results['horizon'] == 1].head(168).dropna()  # First week of t+1 predictions\n",
    "ax4.plot(range(len(sample)), sample['y_actual'], 'b-', linewidth=1, label='Actual')\n",
    "ax4.plot(range(len(sample)), sample['y_pred'], 'r--', linewidth=1, alpha=0.7, label='Predicted')\n",
    "ax4.set_xlabel('Sample Index')\n",
    "ax4.set_ylabel('CMG (USD/MWh)')\n",
    "ax4.set_title('Sample Time Series: t+1 Predictions', fontweight='bold')\n",
    "ax4.legend()\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. TimeSeriesSplit Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validate_neuralprophet(\n",
    "    df: pd.DataFrame,\n",
    "    n_splits: int = 5,\n",
    "    horizons: List[int] = [1, 6, 12, 24],\n",
    "    test_size: int = 720,  # 30 days\n",
    "    verbose: bool = True\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    Perform TimeSeriesSplit cross-validation for NeuralProphet.\n",
    "    \n",
    "    Args:\n",
    "        df: Full dataset with 'ds' and 'y'\n",
    "        n_splits: Number of CV folds\n",
    "        horizons: Horizons to evaluate\n",
    "        test_size: Hours per test fold\n",
    "        verbose: Print progress\n",
    "    \n",
    "    Returns:\n",
    "        Dict with cross-validation results\n",
    "    \"\"\"\n",
    "    tscv = TimeSeriesSplit(n_splits=n_splits, test_size=test_size)\n",
    "    \n",
    "    cv_results = {h: [] for h in horizons}\n",
    "    \n",
    "    for fold, (train_idx, test_idx) in enumerate(tscv.split(df)):\n",
    "        if verbose:\n",
    "            print(f\"\\n{'='*60}\")\n",
    "            print(f\"FOLD {fold + 1}/{n_splits}\")\n",
    "            print(f\"{'='*60}\")\n",
    "        \n",
    "        df_train_fold = df.iloc[train_idx].reset_index(drop=True)\n",
    "        df_test_fold = df.iloc[test_idx].reset_index(drop=True)\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"Train: {len(df_train_fold)} hours ({df_train_fold['ds'].min()} to {df_train_fold['ds'].max()})\")\n",
    "            print(f\"Test:  {len(df_test_fold)} hours ({df_test_fold['ds'].min()} to {df_test_fold['ds'].max()})\")\n",
    "        \n",
    "        # Train model\n",
    "        model = create_neuralprophet_model()\n",
    "        model.fit(df_train_fold, freq='H')\n",
    "        \n",
    "        # Evaluate\n",
    "        fold_results = true_out_of_sample_forecast(\n",
    "            model=model,\n",
    "            df_train=df_train_fold,\n",
    "            df_test=df_test_fold,\n",
    "            horizons=horizons,\n",
    "            sample_every=12,\n",
    "            verbose=False\n",
    "        )\n",
    "        \n",
    "        # Calculate MAE per horizon\n",
    "        for h in horizons:\n",
    "            h_results = fold_results[fold_results['horizon'] == h].dropna()\n",
    "            if len(h_results) > 0:\n",
    "                mae = h_results['abs_error'].mean()\n",
    "                cv_results[h].append(mae)\n",
    "                if verbose:\n",
    "                    print(f\"  t+{h} MAE: ${mae:.2f}\")\n",
    "    \n",
    "    # Summary\n",
    "    summary = {}\n",
    "    for h in horizons:\n",
    "        if cv_results[h]:\n",
    "            summary[f't+{h}'] = {\n",
    "                'mae_mean': np.mean(cv_results[h]),\n",
    "                'mae_std': np.std(cv_results[h]),\n",
    "                'mae_folds': cv_results[h]\n",
    "            }\n",
    "    \n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run cross-validation (this takes a while)\n",
    "print(\"=\"*70)\n",
    "print(\"TIMESERIESSPLIT CROSS-VALIDATION\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nThis validates model performance across multiple time periods.\")\n",
    "print(\"Each fold uses earlier data to train and predicts on later data.\")\n",
    "\n",
    "# Use smaller sample for demo - increase for production\n",
    "cv_summary = cross_validate_neuralprophet(\n",
    "    df=df,\n",
    "    n_splits=3,  # 5 folds for production\n",
    "    horizons=[1, 6, 12, 24],\n",
    "    test_size=720,  # 30 days per fold\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-validation summary\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CROSS-VALIDATION SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "cv_table = []\n",
    "for horizon, stats in cv_summary.items():\n",
    "    cv_table.append({\n",
    "        'Horizon': horizon,\n",
    "        'MAE Mean': f\"${stats['mae_mean']:.2f}\",\n",
    "        'MAE Std': f\"${stats['mae_std']:.2f}\",\n",
    "        '95% CI': f\"${stats['mae_mean'] - 1.96*stats['mae_std']:.2f} - ${stats['mae_mean'] + 1.96*stats['mae_std']:.2f}\"\n",
    "    })\n",
    "\n",
    "cv_df = pd.DataFrame(cv_table)\n",
    "print(\"\\n\" + cv_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Comparison with Original (Leaky) Evaluation\n",
    "\n",
    "**Important**: The original notebook reported MAE of ~$10.88 USD/MWh.\n",
    "\n",
    "After removing data leakage, the true performance is significantly different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary comparison\n",
    "print(\"=\"*70)\n",
    "print(\"COMPARISON: ORIGINAL vs CORRECTED EVALUATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\n ORIGINAL EVALUATION (with data leakage):\")\n",
    "print(\"-\" * 50)\n",
    "print(\"  Test size: 24 hours\")\n",
    "print(\"  Method: Rolling forecast feeding ACTUAL values back\")\n",
    "print(\"  Reported MAE: ~$10.88 USD/MWh\")\n",
    "print(\"  Issue: 23/24 predictions contaminated with future data\")\n",
    "\n",
    "print(\"\\n CORRECTED EVALUATION (no leakage):\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"  Test size: {TEST_DAYS} days ({test_size} hours)\")\n",
    "print(\"  Method: True out-of-sample (using PREDICTED values for AR)\")\n",
    "print(f\"  Corrected MAE: ${overall_mae:.2f} USD/MWh\")\n",
    "print(\"  Validation: TimeSeriesSplit cross-validation\")\n",
    "\n",
    "print(\"\\n KEY FINDING:\")\n",
    "print(\"-\" * 50)\n",
    "improvement = (overall_mae - 10.88) / 10.88 * 100\n",
    "if improvement > 0:\n",
    "    print(f\"  True MAE is {improvement:.0f}% HIGHER than originally reported\")\n",
    "    print(\"  The original '3x improvement' claim was due to methodology error\")\n",
    "else:\n",
    "    print(f\"  True MAE is {-improvement:.0f}% lower than originally reported\")\n",
    "    print(\"  The model performs better than expected even without leakage\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save corrected results\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "results_summary = {\n",
    "    'evaluation_date': datetime.now().isoformat(),\n",
    "    'methodology': 'true_out_of_sample',\n",
    "    'test_days': TEST_DAYS,\n",
    "    'test_hours': test_size,\n",
    "    'overall_mae': overall_mae,\n",
    "    'overall_rmse': overall_rmse,\n",
    "    'metrics_by_horizon': metrics_by_horizon,\n",
    "    'cross_validation': {\n",
    "        horizon: {\n",
    "            'mae_mean': float(stats['mae_mean']),\n",
    "            'mae_std': float(stats['mae_std'])\n",
    "        }\n",
    "        for horizon, stats in cv_summary.items()\n",
    "    },\n",
    "    'notes': [\n",
    "        'This evaluation removes the data leakage from the original notebook',\n",
    "        'Rolling forecast uses PREDICTED values instead of ACTUAL values',\n",
    "        'TimeSeriesSplit cross-validation validates across multiple time periods'\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Save to file\n",
    "output_path = '../logs/neuralprophet_corrected_evaluation.json'\n",
    "os.makedirs('../logs', exist_ok=True)\n",
    "with open(output_path, 'w') as f:\n",
    "    json.dump(results_summary, f, indent=2, default=str)\n",
    "\n",
    "print(f\"Results saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Conclusions\n",
    "\n",
    "### Key Findings:\n",
    "\n",
    "1. **Original evaluation was invalid** due to data leakage in the rolling forecast\n",
    "2. **True MAE is significantly higher** than the originally reported ~$10.88\n",
    "3. **The claimed \"3x improvement\"** over production models was an artifact of flawed methodology\n",
    "\n",
    "### Methodology Fixes Applied:\n",
    "\n",
    "1. ✅ **Removed data leakage**: Rolling forecast now uses PREDICTED values for AR continuity\n",
    "2. ✅ **Expanded test set**: 60 days (1,440 hours) instead of 24 hours\n",
    "3. ✅ **Added cross-validation**: TimeSeriesSplit with k=3-5 folds\n",
    "4. ✅ **Per-horizon metrics**: Separate MAE for t+1, t+6, t+12, t+24\n",
    "\n",
    "### Recommendations:\n",
    "\n",
    "- Compare these corrected results with production ML models using identical methodology\n",
    "- See `model_comparison_fair.ipynb` for side-by-side comparison\n",
    "- Consider NeuralProphet as ensemble member only if it shows genuine improvement"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
